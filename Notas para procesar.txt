
*Preguntarle a Fabián sobre algún proyecto paralelizable para CUDA. ¿Quizás replicar algún paper con dependencia espacial?

Mandarle un mail a Pablo Cornaglia diciendole que queremos estar en FISCOM

Hay que trabajar con una versión vieja de CUDA en el cluster

Para verificar los drivers y las características de la placa sin necesidad de usar CUDA
nvidia-msi

MakeFile tutorial
https://makefiletutorial.com/#getting-started

En el código multmal_solucion se resuelve la cuenta usando una librería de CUDA
Le podemos decir cuántas GPUs usar

Dependiendo de la función que usemos nos va a pedir que las matrices estén cargadas en CPU o GPU

ifdef significa que voy a compilar con opciones. Si defino un macro SIMPLECPU se va a compilar esa parte. De esta forma puedo tener muchos programas en un único programa
Luego para correr cada parte hay que compilar con esas opciones ej: nvcc ... -SIMPLECPU -o ...


COPIAR LOS GOOGLE COLABS DE LA MATERIA


No hay que optimizar prematuramente. Podemos terminar con un código re optimizado de cosas que no eran necesario optimizar


Profiling en python
Podría agregar relojes manuales antes y después de cada función. O podría usar librerías que se encargan de hacer todo por mi. Le pregunté a ChatPGT y me comentó sobre las librerías cProfile y profile. Además, hay herramientas de visualización del profiling mejor interpretables que el texto completo.

ME QUEDARON DUDAS DE LA CLASE 5. ESTÁN EN EL POWER POINT

Intentar optimizar mi código de Python usando Numba

Actividades:
*Decargar todos los google colabs
*Pasar comentarios a pdf clase 7
*Comitear el github
*Pasar preguntas ANKI
*Hacer preguntas ANKI de las primeras clases


Dentro de los kernels en Numba no se puede meter cualquier cosa porque corre en el device. Entonces no puedo pedir desde el kernel acceder a numpy porque no está en la GPU



