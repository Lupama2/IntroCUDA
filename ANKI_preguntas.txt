Clase 0:
Q:
¿Por qué las GPU adquieron importancia en los últimos años?

A:
Por razones físicas con los años las CPU no mejoran significativamente el tiempo de cómputo secuencial. Mientras que las CPU sí pueden mejorar el tiempo de cómputo paralelizando el código, sirviendo como aceleradoras de cálculo.

-----
Además, en estas últimas aumentó progresivamente la velocidad de transferencia de datos.

Clase 1:
Q:
¿Cuáles son los desafíos a la hora de paralelizar una tarea? ¿Es necesario una GPU para paralelizar un código?
A:
*Coordinar múltiples procesos
*Asegurar consistencia de los datos
*Minimizar "overheads" asociados a la comunicación y sincronización. Cuando el problema es lo suficientemente grande los costos se amortizan

Q:
¿En un programa en paralelo, en qué orden se ejecutan las tareas?

A:
Las tareas en paralelo no se ejecutan en algún orden determinado, justamente pedir un orden es romper el paralelismo

Q:
¿Es fácil encontrar la óptima paralelización de una actividad?

A:
Encontrar la óptima paralelización es difícil, pero encontrar una buena es sencillo

Q:
¿Qué tipos de arquitectura en paralelo existen?

A:
*Multi-node con memoria distribuída: Computadoras en paralelo conectas entre sí mediante cables ethernet
*Multiprocessor con memoria compartida: empleando una CPU multicore
*Arquitectura paralela híbrida: empleando CPU y GPU conectadas. Emplear esta arquitectura consituye el paradigma de computación heterogénea.

Q:
¿Existe un límite a cuánto trabajo en paralelo le podemos enviar a la GPU?

A:
Hay un límite superior a la cantidad de trabajo que le podemos mandar. Superado ese punto la GPU serializa, aumentando el tiempo de cómputo.

Q:
¿Qué es CUDA? ¿Existen otras alternativas?

A:
CUDA es una plataforma para la computación heterogénea CPU+GPU, aplicable solo a GPUs de NVidia. Existen librerías y lenguajes de más alto nivel, aunque no permiten todas las optimizaciones posibles con CUDA C/C++

-----
En CUDA es posible ejecutar código en CPU y GPU al mismo tiempo.

Q:
¿Cómo se organizan los hilos en CUDA?

A:
Cada hilo forma parte de un bloque y cada bloque, de una grilla. Las grillas son conjuntos de bloques 1D, 2D o 3D y los bloques son conjuntos de hilos 1D, 2D o 3D. Luego, dependiendo de la GPU, los bloques se "desparraman" de forma distinta, pero nosotros no tenemos que preocuparnos por eso.

Q:
¿Cómo se organizan las memorias en CUDA?

A:
Existe una jerarquía de memorias que van desde las más rápidas, chicas y privadas a las más lentas, grandes y públicas. En ese orden están distribuídos secuencialmente los hilos, bloques y grillas.

Q:
¿Cuántos hilos por bloque usar?
A:
A priori, cualquier cantidad menor a 1024 hilos. Este límite se debe a que se espera que todos los hilos de un bloque se encuentren en el mismo processor core (hardware). Si esto no es suficiente se pueden usar muchos bloques, cuyo límite superior es bastante grande. En términos de optimización, la GPU tiene "números mágicos" para la cantidad de hilos por bloques, que son múltiplos de 32.

-----
Para tener N hilos totales y M hilos por bloque, el nro total de bloques debe ser
int((N + M - 1)/M) 


Q:
¿Cómo cooperan los hilos entre sí?

A:
*Los hilos de un mismo bloque pueden cooperar sincronizando su ejecución, compartiendo datos a través de la shared-memory (de baja latencia).
*Los hilos de distintos bloques no pueden cooperar
Q:
En CUDA C, ¿cómo accedemos a los datos desde un kernel?
A:
Hay que mapear índices de hilos a datos. Para esto, los kernels disponen de variables reservadas, identificadores de los hilos. El indexado de hilos se organiza en grillas-de-bloques-de-hilos, de 1d, 2d o 3d.

-----
Las variables reservadas son threadIdx, blockIdx, blockDim, gridDim.

Q:
En CUDA C/C++, ¿cómo trabajo sobre la memoria global para operar sobre arrays declarados en el host?

A:
1. Alocar memoria en device
2. Copiar de host a device
3. Operar (ejecutando un kernel)
4. Copiar (solo el resultado) del device al host
5. Liberar memoria de device

Q:
¿Cuál es el primer paso a la hora de optimizar un código?

A:
Primero hay que identificar qué tarea exige mayor tiempo de cómputo. Para esto es útil hacer profiling del código. Esto se puede hacer poniendo un montón de timers por el código o usando librerías. No hay que optimizar prematuramente porque se puede terminar con un código con tareas muy optimizadas que no eran necesario optimizar

Q:
¿Cuáles son las 3 reglas para GPGPU Programming?
A:
1. Traer datos a la GPU y mantenerla allí
2. Dar a la GPU suficiente trabajo para hacer
3. Focalizarse en reusar los datos sobre la GPU para evitar memory bandwidth limitations.

Q:
¿Qué son los CUDA Kernels?

A:
Son funciones con restricciones:
* Pueden acceder solo a la memoria del device
* Deben retornar un tipo void
* No soportan un nro variable de argumentos
* No soportan variables estáticas
* No soportan punteros a funciones
* Tienen un comportamiento asincrónico


Clase 2y3:
Q:
Se ejecutó un programa en serie y en paralelo y se obtuvieron los siguientes tiempos de cómputo en función de un tamaño característico N del problema
(figura diap 21)
1. ¿A quién se debe cada curva?
2. ¿Por qué se exhiben estos comporamientos?

A:
1. Violeta: GPU, Verde: CPU
2. La CPU trabaja en serie, de modo que a mayor tamaño, mayor tiempo de cómputo. Mientras que la GPU trabaja en paralelo, de modo que a mayor tamaño, igual tiempo. Cuando el tamaño es demasiado grande, la GPU serializa y el tiempo de cómputo aumenta como en la CPU, pero siendo aún así órdenes de magnitud menor que el de la CPU. Siempre hay un costo asociado a la creación de los kernels y otras inicializaciones que se amortizan solo si el tamaño es grande. Es por esto que inicialmente la GPU tarda más que la CPU

Q:
¿Por qué importa ejecutar los programas de forma más rápida?

A:
Porque a modo gral vamos a equivocarnos, así que mejor si nos equivocamos rápido

Clase 4:
Q:
¿Cuáles son los distintos tipos de memorias en CUDA? Describí brevemente cada una de ellas

A:
* Memoria Global: es la más grande y la más lenta. Puede ser leída y escrita por la CPU y por los threads de GPU. Permite comunicar datos entre CPU y GPU
* Memoria Constante: es parte de la memoria global. CPU puede leer y escribir, y es sólo de lectura para los threads
* Memoria compartida: es pequeña y muy rápida y es compartida por todos los threads de un bloque. Es de lectura/escritura por los threads. Puede comunicar datos entre threads del mismo bloque.
* Registros: aquí se guardan las variables que se declaran en un kernel y son privadas a cada hilo. El programador no tiene control explícito esta memoria.
* Memoria local: es usada por el compilador automáticamente para alojar variables cuando hace falta. Fisicamente es la global.
* Memoria de textura: es controlada por el programador y puede beneficiar aplicaciones con localidad espacial donde el acceso a memoria global es un cuello de botella.

Q:
Si no queremos usar kernels en CUDA C/C++, ¿qué librerías de más alto nivel podemos emplear?
A:
*CUBLAS
*CUBLASXt

Q:
¿Qué es Cupy?
A:
Es una librería de python con interfaz similar a numpy que permite realizar cálculo en paralelo de alto nivel. También permite hacer tareas de bajo nivel, como definir kernels, hilos, bloques, etc.

Q:
¿Qué es una "reducción"?

A:
Es una operación distributiva y asociativa que es paralelizable. Por ejemplo, para sumar un vector de N componentes se puede sumar de a pares, creando un vector de N/2 componentes y así sucesivamente. De este modo, los threads colaboran entre sí

Q:
¿Es posible sincronizar los hilos?

A:
Solo se pueden sincronizar los hilos que pertenecen a un mismo bloque, no así con hilos que pertenecen a distintos bloques.

Clase 5:
Q:
¿Qué es el "precalentamiento" de un kernel? ¿Por qué es necesario hacerlo?
A:
Si queremos medir tiempo de cómputo, queremos evitar medir el tiempo de compilación e inicialización del kernel. Es por eso que es necesario hacer un precalentamiento del mismo, ejecutándolo una vez (o varias veces) antes de utilizarlo

Q:
¿Qué es thrust y cuál es la ventaja de emplearlo respecto a CUDA C directamente?
A:
Thrust es una librería de alto nivel para C++ de algoritmos en paralelo. Es útil para tareas que se pueden descomponer en "parallel patterns" fundamentales y permite reducir la escritura de código. Tiene la ventaja de ser portable y poder compilarse y ejecutarse en:
*Serial en CPU
*Paralelo en CPU
*Paralelo en GPU

Clase 7:
Q:
¿Se pueden optimizar códigos de Python usando GPU?
A:
Sí! Se pueden aprovechar librerías como Cipy, Rapids, PyCUDA y Numba. Esta última tiene la particularidad de que tmb permite acelerar códigos que corren en CPU

Q:
¿Por qué los resultados pueden dar distintos en GPU vs CPU?
A:
La aproximación de floats es distinta en device y host

Q:
¿Es válido medir el tiempo de ejecución de un código en GPU a partir de una única corrida?
A:
No. Hay que hacer estadística de los tiempos de ejecución. Además, la primera vez tarda más por lo que es necesario hacer un "precalentamiento" que no debe ser considerado en el código

Q:
¿Qué es Julia? ¿Cómo se compara con python?

A:
Julia es un lenguaje de programación útil for numerical computing and scientific computing tasks that require high performance and parallelism, while Python is a good choice for general-purpose programming, web development, and data analysis tasks that require a large ecosystem of libraries and tools.


HASTA ACÁ LLEGUÉ


Clase 8 o 9:


Q: ¿Qué es OpenACC? ¿Sobre qué placas corre? ¿Para qué sirve?

A:
Hay que saber identificar el paralelismo

Q: ¿Cuál es el problema de eficiencia que suele ocurrir con OpenAcc? ¿Cómo se puede solucionar?

A: Hay un excesivo movimiento de datos por copias de GPU a CPU y visceversa. Las copias innecesarias pueden evitarse usando data regions. Estas definen regiones del código en las cuales los arrays de GPU se quedan en la GPU y son compartidos entre los kernels de la region. Solo salen de la GPU cuando sale de la región.

Q: ¿Cuáles son las "Three rules of GPGPU Programming"? ¿Qué significa GPGPU?


A: Observation has shown that there are three general rules to creating high-performance GPGPU programs:
1. Get the data on the GPGPU and keep it there
2. Give the GPGPU enough work to do
3. Focus on data reuse within the GPGPU to avoid memory bandwidth limitations

GPGPU significa General-Purpose computing on Graphics Processing Units



Cuando con CuPy hago el producto de una matriz definida como sparse con un vector denso, la librería se da cuenta que la matriz es sparse y usa el producto más eficiente

Copiar los google colabs

Ver cómo correr el proyecto de clases en el cluster


Clase n:
Q: ¿Se puede optimizar un producto matricial?

A: Sí. Existen múltiples formas de hacerlo, ya se en GPU o CPU. Existen librerías de bajo y alto nivel que realizan tales operaciones de forma óptima, en base a las propiedades de las matrices (como si son sparse)
-----
Los productos matriciales aparecen en muchos modelos físicos, como en solución de ecuaciones en derivadas parciales, grafos, modelos de epidemias, etc.

Clase n+1:

Q: ¿Cómo puedo resolver una ecuación diferencial que se puede separar en sus modos normales?

A: Con transformada de Fourier! Se puede implementar fácilmente en GPU mediante la librería Cufttp.

Clase 10 u 11:

Q:
¿Cuál es la diferencia entre generar nros random en CPU y GPU?
A:

Q:
¿Cuáles son los problemas asociados a usar GPU?

A:
